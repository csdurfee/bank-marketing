{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Marketing Analysis\n",
    "\n",
    "by Casey Durfee <casey.durfee@colorado.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I took a look at the \"Bank Marketing\" dataset in the  UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#\n",
    "\n",
    "The data covers a bank's outbound marketing efforts to over 40,000 customers, with some information about them and the state of the economy at the time they were contacted, and whether they bought the product (a term deposit account) or not.  There are 20 features for each customer. I will go through each of the 20 features below and describe what they are, and what the data looks like.\n",
    "\n",
    "My goal will be to build a Machine Learning model to predict who is likely to buy. I will look at a few of the algorithms covered in this class (random forest, SVM, gradient boosting) and determine which one does the best job of making predictions on this dataset. I will then optimize whichever algorithm shows the best results using grid search. Part of my criteria here is which algorithms can actually run in a timely fashion on my machine, so I will be looking for ways to optimize.\n",
    "\n",
    "My motivation is that a good model could help the bank spend its sales and marketing budget more effectively and be used to predict the success of using outbound marketing to promote new products.\n",
    "\n",
    "A more complete version of this data was used in the following research paper:\n",
    "\n",
    "```[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014```\n",
    "\n",
    "That paper is available online here: https://www.sciencedirect.com/science/article/pii/S016792361400061X#t0010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this code locally, you will need to install the required libraries. If you don't want to junk up your global dependencies, you can use virtualenv.\n",
    "\n",
    "1. download this repo\n",
    "2. go to the downloaded directory\n",
    "3. `virtualenv venv`\n",
    "4. `source venv/bin/activate`\n",
    "5. `pip install -r requirements.txt`\n",
    "6. go make a cup of coffee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>43</td>\n",
       "      <td>management</td>\n",
       "      <td>divorced</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>93.2</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>4.191</td>\n",
       "      <td>5195.8</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age         job   marital    education default housing loan   contact  \\\n",
       "1343   43  management  divorced  high.school      no     yes   no  cellular   \n",
       "\n",
       "     month day_of_week  ...  campaign  pdays  previous     poutcome  \\\n",
       "1343   nov         mon  ...         2    999         0  nonexistent   \n",
       "\n",
       "     emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
       "1343         -0.1            93.2          -42.0      4.191       5195.8  no  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## if true, this will automatically download & cache the data.\n",
    "USE_CACHE = True\n",
    "\n",
    "if USE_CACHE:\n",
    "    import requests_cache\n",
    "    import zipfile\n",
    "    import io\n",
    "    session = requests_cache.CachedSession(\"http_cache\", \n",
    "                                            backend=\"filesystem\")\n",
    "    zipped = session.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip\")\n",
    "    z = zipfile.ZipFile(io.BytesIO(zipped.content))\n",
    "    raw_data = z.open(\"bank-additional/bank-additional.csv\")\n",
    "    bank_data = pd.read_csv(raw_data, delimiter=\";\")\n",
    "\n",
    "else:\n",
    "    ## file obtained from https://archive.ics.uci.edu/ml/machine-learning-databases/00222/\n",
    "    # you will need to manually download & extract bank-additional.zip in this dir\n",
    "    bank_data = pd.read_csv(\"bank-additional-full.csv\", delimiter=\";\")\n",
    "\n",
    "bank_data.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Exploration & Cleaning\n",
    "\n",
    "It's always good to start by checking whether any of the columns have null values. They don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in bank_data.columns:\n",
    "    if bank_data[col].isnull().any():\n",
    "        print(f\"Nulls in {bank_data[col]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variables\n",
    "\n",
    "Next, let's check the categorical variables, such as `job`, `marital`, `education`, `default`, `housing`, `loan`, and `contact`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in bank_data.columns:\n",
    "    if bank_data[col].dtype == 'object':\n",
    "        print(bank_data[col].value_counts())\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unknown Values in Categorical Variables\n",
    "\n",
    "Many of the categorical variables about the customer's demographics have 'unknown' values: `job`, `marital`, `education`, `default`, `housing`, `loan`. (for reference, `default` is whether the customer is in default on any loans. `housing` is whether the customer has a mortgage with the bank. `loan` is whether the customer has a non-housing loan.)\n",
    "\n",
    "While unknown values are not ideal, I don't believe it makes sense to throw out these values. If it turns that customers with `unknown` values are less likely to buy the product, that says something too: that the business should stop trying to market to customers that they don't have enough demographic information about. In other words, `unknown` conveys less information than the other categorical values, but not zero information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are the Categorical Variables Reasonable?\n",
    "\n",
    "`poutcome` is whether previous marketing campaigns to this person were successful. It looks like this is a very strong predictor of whether this campaign will be successful, so it will be interesting to see if this is used by the winning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success rate for customers who have been previously marketed to successfully: 64.7887323943662 %\n",
      "lift is 5.917179351050874\n"
     ]
    }
   ],
   "source": [
    "bank_data.poutcome.value_counts()\n",
    "\n",
    "success_and_yes = len(bank_data[ (bank_data.poutcome == 'success') & (bank_data.y == 'yes')])\n",
    "success_and_no = len(bank_data[ (bank_data.poutcome == 'success') & (bank_data.y == 'no')])\n",
    "\n",
    "poutcome_rate = success_and_yes / (success_and_yes + success_and_no)\n",
    "\n",
    "overall_rate = len(bank_data[bank_data.y == 'yes']) / len(bank_data)\n",
    "\n",
    "print(f\"success rate for customers who have been previously \" + \\\n",
    "      f\"marketed to successfully: {100 * poutcome_rate} %\")\n",
    "\n",
    "print(f\"lift is {poutcome_rate / overall_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a big lift.\n",
    "\n",
    "Other than that, this field is just a yes/no value, so there are no outliers to look at, and without domain knowledge, it's hard to draw any conclusions about whether there are bad datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date fields\n",
    "\n",
    "`month` and `day_of_week` indicate when the customer was contacted.\n",
    "\n",
    "Note that for `month` and `day_of_week` it doesn't make sense to convert them to numbers. 'February' isn't January +1 within the context of this model (there's no reason to believe that jan < feb < mar < ... as far as success rate). So we should treat them as categorical variables even though they do have a 'natural' numerical equivalent.\n",
    "\n",
    "Because this data is over an irregular span of time, the days of the week are evenly distributed, but the months are not. Over a third of the calls are from the month of May. I believe this strongly limits the value of this info, and in the real world, turning time series data into categorical variables like this would be a bad idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data['month'].value_counts().plot.bar()\n",
    "\n",
    "len(bank_data[bank_data.month == 'may']) / len(bank_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `y` and class imbalance\n",
    "\n",
    "`y` is whether the sales attempt was successful or not. Converting `y` is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data['y'] = bank_data['y'].replace({'no': 0, 'yes': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 90% of sales calls are unsuccessful. That's a pretty big class imbalance, which means we need to be careful with how we select and score our model, because a model that always predicts `no`, regardless of the feature data, would be accurate 89% of the time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100 * sum(bank_data.y == \"no\") / len(bank_data.y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn's `DummyClassifier` always predicts the majority class, so we'll use it as a baseline comparison to keep us honest when comparing models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting Categoricals to Dummy Variables\n",
    "\n",
    "If a categorical variable has `n` possible values, only `n-1` dummy variables are needed to encode the information, since the last category can be inferred from the value of the other categories.\n",
    "\n",
    "So I am converting each categorical variable to dummy variables, then dropping the least frequent dummy variable. `get_dummies` has an option to drop the first dummy variable, but I think dropping the least frequent makes the model marginally more understandable, because on most of these variables, \"unknown\" is the least frequent option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in bank_data.columns:\n",
    "    if bank_data[field].dtype == 'object':\n",
    "        dummies = pd.get_dummies(bank_data[field], prefix=field)\n",
    "        toss_variable = f\"{field}_{bank_data[field].value_counts().idxmin()}\"\n",
    "\n",
    "        bank_data = pd.concat([bank_data, dummies], axis='columns') \\\n",
    "                        .drop([field, toss_variable], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical values\n",
    "\n",
    "Now, let's look at the rest of the non-categorical values. We mostly get numerical data about previous contacts with the customer, and about the economy at the time of contact. The original paper analyzed more demographic data, such as the customer's net worth, but here we are only given age. The range and distribution look reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bank_data.age.describe())\n",
    "\n",
    "bank_data.age.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previous Contacts Data\n",
    "\n",
    "We are given several fields about previous contacts with the customer.\n",
    "\n",
    "`pdays` is the number of days since last contact, and if there hasn't been a previous contact, the value 999 is assigned. It looks like this is the most common value in the column, indicating most people haven't been contacted before.\n",
    "\n",
    "The column `previous` indicates the number of previous contacts with the customer. This column should always be 0 if `pdays` is 999. Let's make sure that's true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possibly_bad_pdays = bank_data[(bank_data.pdays == 999) & (bank_data.previous > 0) ]\n",
    "\n",
    "print(\"possibly bad 'pday' values: %.2f %%\" % (100 * len(possibly_bad_pdays.previous) / len(bank_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! Almost 10% of the rows have a value that we know has to be wrong. So I'm going to to toss `pdays`.\n",
    "\n",
    "The documentation cautions that the  `duration` column is not a realistic column to use for a predicative model, so it's gone as well:\n",
    "\n",
    "> this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "\n",
    "Despite the caution, they used this field in their analysis, and unsurprisingly, it was one of the strongest predictors they found! On top of that, 4 of the top 5 strongest predictors they found were not included in the public dataset, presumably for privacy or proprietary reasons. So I won't be able to compare my models to theirs, because I don't have the same data they did, and are trying to make predictions rather than post-hoc explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data = bank_data.drop(['pdays', 'duration'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`campaign` is the \"number of contacts performed during this campaign and for this client (numeric, includes last contact)\". It has an extremely long tail, which is unusual, and worth checking out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.campaign.plot.box()\n",
    "\n",
    "print(bank_data.campaign.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The largest value is 56 times. That seems like a huge number of contacts to one person before they finally said \"yes\" or \"no\". Does the distribution of this data make sense?\n",
    "\n",
    "Intuitively, it seems like `campaign` should follow an exponential distribution. We can use a qq plot to verify this. Does this field act like a prototypical exponential distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(bank_data.campaign, fit=True, line='r', dist=stats.expon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we'd like to see a flat line there. So it doesn't (based on the eye test, at least) appear to be an exponential distribution, because the tail is too long, but it's close.  The `gamma` distribution fits better, since it's just a more generalized version of the exponential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(bank_data.campaign, fit=True, line='r',dist=stats.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a pretty good fit, so for now I'm going to assume that the data in the `campaign` field is organic, rather than being due to some type of error. I think it would be justified to remove the one extreme outlier (in the top right of the chart above) because it clearly diverges from the other 41,000 data points. But it's also unlikely to have a major impact given the number of samples and other features in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Economic Factors Data\n",
    "\n",
    "The remaining variables are all macroeconomic statistics at the time the customer was contacted: `emp.var.rate`, `cons.price.idx`, `cons.conf.idx`, `euribor3m` and `nr.employed`.\n",
    "\n",
    "We need to check correlations between them. Although we've been given 5 metrics here, are we really getting 5 unique dimensions of data, or can we eliminate some?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_predictors = ['emp.var.rate', 'cons.price.idx', 'cons.conf.idx',\n",
    "                    'euribor3m', 'nr.employed']\n",
    "\n",
    "bank_data.loc[:, macro_predictors].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`emp.var.rate`, `euribor3m` and `nr.employed` are very highly correlated. This is a bit surprising since `euribor3m` is the European inter-bank lending rate, `emp.var.rate` is the employment rate, and `nr.employed` is the number of employees. \n",
    "\n",
    "The strong correlation between lending rate and employment rate is curious, but may be explained by the time that this data came from, which was May 2008 to November 2010. That was around the period of the global economic crisis at the end of 2008, which makes me question how valuable this economic data is for predictions in the future. \n",
    "\n",
    "Since the correlations are so high, I am only going to use the `euribor3m` rate. I will also keep `cons.price.idx` (consumer price index) and `cons.conf.idx` (consumer confidence index), since they are less correlated, so can potentially add additional value to being in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data = bank_data.drop(['emp.var.rate', 'nr.employed'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these categorical variables don't have a natural order to them, so they need to be encoded as dummy variables. For example, for the `marriage` field, instead of arbitrarily assigning integer values to `{single, married, divorced, unknown}`, I'm adding variables like `{is_single, is_married, is_divorced, is_unknown}` that will be 1 or 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = bank_data['y'].copy()\n",
    "x = bank_data.copy().drop(['y'], axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model: Random Forest\n",
    "\n",
    "\n",
    "A random forest classifier seems like a good first option, since it doesn't require normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(random_state=0, class_weight='balanced')\n",
    "rf_model.fit(x_train, y_train)\n",
    "\n",
    "print(f\"Random Forest accuracy: {rf_model.score(x_train, y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest classifier looks pretty good. 99.4% accurate on the training set. \n",
    "\n",
    "However, it's not significantly better than a dummy classifier as far as accuracy on the test set. This illustrates how accuracy isn't the right metric to use when the classes are imbalanced. Based on accuracy alone, there's no real difference in the two models, even though, if nothing else, the random forest model is clearly doing a lot more work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "dummy_model = DummyClassifier().fit(x_train, y_train)\n",
    "dummy_pred = dummy_model.predict(x_test)\n",
    "\n",
    "rf_pred = rf_model.predict(x_test)\n",
    "\n",
    "print(f\"Random Forest Model accuracy: {rf_model.score(x_test, y_test)}\")\n",
    "#print(classification_report(y_test, rf_pred))\n",
    "\n",
    "\n",
    "print(f\"Dummy Model accuracy: {dummy_model.score(x_test, y_test)}\")\n",
    "#print(classification_report(y_test, dummy_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC AUC - Why?\n",
    "\n",
    "ROC is essentially a plot of the ratio of true positive rate to false positive rate. It can show us what accuracy does not.\n",
    "\n",
    "Since the dummy classifier always says yes, the True Positive Rate and False Positive Rate are both 1 (the dummy classifier is a very upbeat classifier). So the AUC will always be .5 for the dummy classifier -- it's the triangle defined by $\\{(0,0), (1,1), (1,0)\\}$.\n",
    "\n",
    "So any model that's better at producing true positives than false positives will have an AUC over .5, with the perfect classifier having an AUC of 1.\n",
    "\n",
    "Instead of accuracy, for the rest of this notebook, I will use the AUC of the ROC curve, which is a much better measure of how relatively good two models are. (This was also the approach of the authors of the original paper.)\n",
    "\n",
    "Let's calculate the ROC AUC for Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, rf_pred)\n",
    "roc_auc = roc_auc_score(y_test, rf_pred)\n",
    "\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it beats the dummy model's score of .5. But surely we can do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Model: Gradient Boosting\n",
    "\n",
    "Gradient Boosting is fast and often performs extremely well, without the need for scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier()\n",
    "gb_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_pred = gb_model.predict(x_test)\n",
    "\n",
    "print(f\"Gradient Boosting ROC AUC: {roc_auc_score(y_test, gb_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, basically equal to random forest. I manually tweaked the parameters, but couldn't do better than .62 with this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Model: Neural Networks.\n",
    "\n",
    "The original paper found a Neural Network model performed the best (though as discussed, they had more/better data than what is in the public dataset; additionally, they were using different machine learning libraries.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn_model = MLPClassifier()\n",
    "nn_scaler = StandardScaler().fit(x_train)\n",
    "nn_model.fit(nn_scaler.transform(x_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_pred = nn_model.predict(nn_scaler.transform(x_test))\n",
    "\n",
    "print(f\"Neural Network ROC AUC {roc_auc_score(y_test, nn_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This did basically the same random forest and gradient boosting, with a slower training time for the model. There's some possible parameter tweaking, but I'm going to save the optimizations for the last model, SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Final Model: SVM\n",
    "For SVM, it's super important to keep training data as small as possible because training time can be prohibitive. Because we have an imbalance of 'yes' and 'no' responses, if we're just training on a sample of the data, we want to make sure it includes as many of the rarer 'yes' responses as possible.\n",
    "\n",
    "sklearn's SVC class does have an option that's supposed to help with class imbalances (`class_weight='balanced'`), but I found it's extremely slow to train on all 40,000 points in this data set (around an hour on my computer.)\n",
    "\n",
    "I was curious if creating a set of data that contains all the 'yes' responses plus an even amount of 'no' responses that were randomly picked would do better than the `class_weight=balanced` option.\n",
    "\n",
    "This is just a naive approach. There are algorithms like [SMOTE](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html) that surely do a better job, but I wanted to get a feel for it myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"THE BIG IDEA: get ALL yeses in the x_train set\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mthen a matching number of nos in the x_train set\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mSO all data is coming from x_train\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mwe can use that to score without \"cheating\".\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m bank_data_in_xtrain \u001b[38;5;241m=\u001b[39m bank_data\u001b[38;5;241m.\u001b[39miloc[bank_data\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39misin(\u001b[43mx_train\u001b[49m\u001b[38;5;241m.\u001b[39mindex)]\n\u001b[1;32m     14\u001b[0m yeses_in_xtrain \u001b[38;5;241m=\u001b[39m bank_data_in_xtrain[bank_data_in_xtrain\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     15\u001b[0m nos_in_xtrain   \u001b[38;5;241m=\u001b[39m bank_data_in_xtrain[bank_data_in_xtrain\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\"\"\"THE BIG IDEA: get ALL yeses in the x_train set\n",
    "then a matching number of nos in the x_train set\n",
    "SO all data is coming from x_train\n",
    "but we are only using a subsample of it with even class membership.\n",
    "\n",
    "Since the model hasn't seen anything in x_test & y_test,\n",
    "we can use that to score without \"cheating\".\n",
    "\"\"\"\n",
    "\n",
    "bank_data_in_xtrain = bank_data.iloc[bank_data.index.isin(x_train.index)]\n",
    "\n",
    "yeses_in_xtrain = bank_data_in_xtrain[bank_data_in_xtrain.y == 1]\n",
    "nos_in_xtrain   = bank_data_in_xtrain[bank_data_in_xtrain.y == 0]\n",
    "\n",
    "even_sample_of_nos = nos_in_xtrain.sample(n=len(yeses_in_xtrain))\n",
    "\n",
    "even_data_svm = pd.concat([yeses_in_xtrain, even_sample_of_nos], \n",
    "                    axis='rows')\n",
    "\n",
    "# shuffle these, so we have the option to take a slice of the data & have it be random\n",
    "## (roughly equal yes & no)\n",
    "## taken from https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows\n",
    "even_data_svm = even_data_svm.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(f\"Length of even SVM training data: {len(even_data_svm)}\")\n",
    "\n",
    "even_y_train  = even_data_svm['y'].copy()\n",
    "even_x_train  = even_data_svm.copy().drop(['y'], axis=1)\n",
    "\n",
    "print(f\"value counts in SVM training data:\\n{even_y_train.value_counts()}\")\n",
    "\n",
    "print(f\"Value counts in the first 1K\\n: {even_y_train[:1000].value_counts()}\")\n",
    "\n",
    "even_scaler = StandardScaler().fit(even_x_train)\n",
    "\n",
    "scaled_even_x_train = even_scaler.transform(even_x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we are training the SVM on a special set of data that has an equal number of yes and no datapoints. However, we are then evaluating it against the same `x_test` sample as the other models, so it's fair to compare the outputs of this models to the others we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = SVC().fit(scaled_even_x_train, even_y_train)\n",
    "\n",
    "even_x_test = even_scaler.transform(x_test)\n",
    "svc_pred = svc_model.predict(even_x_test)\n",
    "print(f\"even inputs SVC ROC score: {roc_auc_score(y_test, svc_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, this is the best model we've found as far as ROC, and we haven't even tuned the hyperparameters. And it only took 14 seconds to `fit()` and `predict()` on my computer. \n",
    "\n",
    "Let's compare this to using the `class_weight='balanced'` feature of SVC and the same training set we used for the Random Forest model. This will take much longer to run, since we are training on around 35,000 points instead of 8,000. On my computer, it takes about 3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_THIS_SLOW_THING = False\n",
    "\n",
    "if RUN_THIS_SLOW_THING:\n",
    "    balanced_scaler = StandardScaler().fit(x_train)\n",
    "    scaled_x_train = balanced_scaler.transform(x_train)\n",
    "\n",
    "    svc_balanced_class_weight = SVC(class_weight='balanced').fit(scaled_x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the resulting model is more complex, it takes much longer to predict and score as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_THIS_SLOW_THING:\n",
    "    svc_class_weight_pred = svc_balanced_class_weight.predict(balanced_scaler.transform(x_test))\n",
    "    print(f\"balanced SVC model ROC score: {roc_auc_score(y_test, svc_class_weight_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran it, the model using the `class_weight='balanced'` got a score of .736, which was just slightly worse than training with the even inputs (.738). Since the even inputs are vastly faster to train on, that's what I will use to tune the SVC hyperparameters.\n",
    "\n",
    "I think I've shown here that the `class_weight='balanced'` is really just a convenience function. it's probably fine for smaller datasets, but for larger datasets, it's very inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Grid Search to Optimize SVC\n",
    "\n",
    "So far, SVC has proven to be the best. Let's see how good we can get it with a little grid search to optimize hyperparameters. For the sake of speed, I am only going to train on 1,000 data points from the `even` test sample.  I shuffled this data, so a slice of 1000 should have roughly even `yes` and `no` datapoints. I will reuse the code we were given in Lab 6 to visualize the results of the grid search.\n",
    "\n",
    "I started with ranges of 2**-5 to 2**5 for both and then kept adjusting them until the values started going down again.\n",
    "\n",
    "Training with the subset is vastly faster than with the 8,000 data points in the `scaled_even_x_train` set. Training with the entire `x_train` set is not feasible with grid search (at least on my 7 year old laptop.)\n",
    "[TODO: fix vergiage]\n",
    "\n",
    "Note that we need to make a custom scorer function for it to use the ROC AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "my_scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "parameters = {\n",
    "    'C': [2**x for x in range(1, 11)],\n",
    "    'gamma': [2**x for x in range(-14,-4)]\n",
    "}\n",
    "\n",
    "\n",
    "# train on just 1000 points -- makes search vastly faster.\n",
    "grid = GridSearchCV(SVC(), parameters, scoring=my_scorer).fit(scaled_even_x_train[-1000:], even_y_train[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the code from the week 6 problem set (slightly adjusted) to display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize\n",
    "\n",
    "class MidpointNormalize(Normalize):\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "\n",
    "def plotSearchGrid(grid):\n",
    "    \n",
    "    scores = [x for x in grid.cv_results_[\"mean_test_score\"]]\n",
    "    scores = np.array(scores).reshape(len(grid.param_grid[\"C\"]), len(grid.param_grid[\"gamma\"]))\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "    plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot,\n",
    "               norm=MidpointNormalize(vmin=0.2, midpoint=0.6))\n",
    "    plt.xlabel('gamma')\n",
    "    plt.ylabel('C')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(np.arange(len(grid.param_grid[\"gamma\"])), grid.param_grid[\"gamma\"], rotation=45)\n",
    "    plt.yticks(np.arange(len(grid.param_grid[\"C\"])), grid.param_grid[\"C\"])\n",
    "    plt.title('Validation accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotSearchGrid(grid)\n",
    "print(np.array([x for x in grid.cv_results_[\"mean_test_score\"]]).reshape(len(grid.param_grid[\"C\"]), \n",
    "                                                                        len(grid.param_grid[\"gamma\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it looks like the optimal score was obtained at C=4, gamma=0.0078125. Let's use these params to train a final SVC model on the entire `even` training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = SVC(C=4, gamma=.0078125).fit(scaled_even_x_train, even_y_train)\n",
    "\n",
    "even_x_test = even_scaler.transform(x_test)\n",
    "tuned_pred = tuned_model.predict(even_x_test)\n",
    "print(f\"even inputs SVC ROC score: {roc_auc_score(y_test, tuned_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! The only problem is that this score is (very slightly) worse than what I got when I ran `SVC()` without any tuning parameters. I think this shows that in this context, the tuning parameters for SVC don't matter that much. I searched thru 10 orders of magnitude for both C and gamma and most of them gave scores around .72. So it seems like we're kind of already getting the most we can out of SVC.\n",
    "\n",
    "I also played around with other kernels. The `poly` gave roughly the same results as the default `rbf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_model = SVC(kernel='poly').fit(scaled_even_x_train, even_y_train)\n",
    "\n",
    "poly_pred = poly_model.predict(even_x_test)\n",
    "print(f\"poly kernel, even inputs SVC ROC score: {roc_auc_score(y_test, poly_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "With the `rbf` kernel we are using, we can't get the feature importance just by looking at the coefficients of the model like we could with a `linear` kernel. However, sklearn does have a tool called permutation importance we can use. Note: we will use the ROC AUC scorer we made earlier here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adapted from https://stackoverflow.com/questions/41592661/determining-the-most-contributing-features-for-svm-classifier-in-sklearn\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "perm_importance = permutation_importance(svc_model, even_x_test[:200], y_test[:200], scoring=my_scorer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(x_train.columns)\n",
    "\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "plt.barh(features[sorted_idx][-5:], perm_importance.importances_mean[sorted_idx][-5:])\n",
    "plt.xlabel(\"Permutation Importance\")\n",
    "plt.show()\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the `euribor3m` rate is the most significant, which is what the original paper also found to be the strongest predictor. `month_oct` being second I think shows the potential for overfitting due to the weird structure of date data (discussed above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Models\n",
    "\n",
    "Let's plot the ROC curve for all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Plot dummy -- AUC should be .5\n",
    "lw = 2\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, dummy_pred)\n",
    "\n",
    "plt.figure()\n",
    "roc_auc = roc_auc_score(y_test, dummy_pred)\n",
    "plt.plot(\n",
    "    fpr,\n",
    "    tpr,\n",
    "    color=\"green\",\n",
    "    lw=lw,\n",
    "    label=\"Dummy (area = %0.2f)\" % roc_auc,\n",
    ")\n",
    "\n",
    "## BOOSTING\n",
    "fpr, tpr, _ = roc_curve(y_test, gb_pred)\n",
    "roc_auc = roc_auc_score(y_test, gb_pred)\n",
    "\n",
    "plt.plot(\n",
    "    fpr,\n",
    "    tpr,\n",
    "    color=\"purple\",\n",
    "    lw=lw,\n",
    "    label=\"Gradient Boosing (area = %0.2f)\" % roc_auc,\n",
    ")\n",
    "\n",
    "\n",
    "## Random Forest\n",
    "fpr, tpr, _ = roc_curve(y_test, rf_pred)\n",
    "roc_auc = roc_auc_score(y_test, rf_pred)\n",
    "\n",
    "plt.plot(\n",
    "    fpr,\n",
    "    tpr,\n",
    "    color=\"darkorange\",\n",
    "    lw=lw,\n",
    "    label=\"Random Forest(area = %0.2f)\" % roc_auc,\n",
    ")\n",
    "\n",
    "\n",
    "## Neural Network\n",
    "fpr, tpr, _ = roc_curve(y_test, nn_pred)\n",
    "roc_auc = roc_auc_score(y_test, nn_pred)\n",
    "\n",
    "plt.plot(\n",
    "    fpr,\n",
    "    tpr,\n",
    "    color=\"purple\",\n",
    "    lw=lw,\n",
    "    label=\"Neural Network(area = %0.2f)\" % roc_auc,\n",
    ")\n",
    "\n",
    "### SVM\n",
    "fpr, tpr, _ = roc_curve(y_test, svc_pred)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, svc_pred)\n",
    "plt.plot(\n",
    "    fpr,\n",
    "    tpr,\n",
    "    color=\"blue\",\n",
    "    lw=lw,\n",
    "    label=\"SVM (area = %0.2f)\" % roc_auc,\n",
    ")\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "SVM definitely did the best, achieving a much higher true positive rate than the others, at the cost of only a slightly higher false positive rate. Out of the box, there was very little difference between random forest, neural network, and boosting on this problem.\n",
    "\n",
    "#### What Worked Well\n",
    "I was able to improve training times of SVM without hurting quality by only working on a subset of the training data with even classes. This performed much, much better than the built-in `balanced` option in the `SVC` class.\n",
    "\n",
    "#### What Didn't Work\n",
    "I wasn't able to significantly improve the SVM performance by using a grid search, perhaps because I had to use subsamples to make the grid search run in a timely fashion. However, for all models I tried against this data set, tuning hyperparameters had little impact on the quality of results.\n",
    "\n",
    "#### potential improvements\n",
    "The biggest potential improvement I see is more analysis of correlations within the categorical variables, and feature reduction in general. This could help the model be faster and more understandable without sacrificing quality of prediction. The initial data had 20 dimensions, and converting to dummy variables blew that up to 50 dimensions.  With 40,000 observations in the dataset, that's over 2 Million points of data total.\n",
    "\n",
    "It would also be interesting to use the SMOTE library rather than my hand-rolled class balancing. There are a wide variety of techniques for dealing with class imbalance that I didn't have time to get into."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e5e67dba9ec50688b51d210d69127116c7da1f67ec0df7da4b6c1a14dc73fba"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('py3-data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
